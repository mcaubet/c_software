#!/bin/bash
#SBATCH --clusters=merlin6
#SBATCH --job-name=get_cpu
#SBATCH --partition=hourly         # Specify 'general' or 'daily' or 'hourly': partition=<general|daily|hourly> 
#SBATCH --time=00:15:00            # Strictly recommended when using 'general': partition: time=<D-HH:MM:SS>
#SBATCH --output=logs/get_cpu.out  # Generate custom output file
#SBATCH --error=logs/get_cpu.err   # Generate custom error  file
#SBATCH --ntasks=2                 # Uncomment and specify #nodes to use  
#SBATCH --cpus-per-task=4          # Uncomment and specify the number of cores per task
#SBATCH --mem-per-cpu=8000         # Maximum memory for each node 352000

###################################################
# Load the environment modules for this job (the order may be important):
module purge
# module load intel/18.4 openmpi/3.1.2
# module load gcc/7.4.0 openmpi/3.1.4
module load intel/15.3 openmpi/1.10.2

EXE="/data/user/caubet_m/Slurm/get_cpu/get_cpu"

# The MPI command to run:
export MPIRUN_OPTIONS="--bind-to hwthread --map-by socket:pe=$SLURM_CPUS_PER_TASK -report-bindings -mca pml ucx"
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export NUM_CORES=$SLURM_NTASKS*$SLURM_CPUS_PER_TASK
echo "${EXECUTABLE} running on ${NUM_CORES} cores with ${SLURM_NTASKS} MPI-tasks and ${OMP_NUM_THREADS} threads"

# mpirun -n $SLURM_NTASKS $MPIRUN_OPTIONS $EXE
srun $EXE 
