#!/bin/bash
#SBATCH --clusters=merlin6
#SBATCH --job-name=get_cpu
#SBATCH --partition=cpu-maint
#SBATCH --account=merlin
#SBATCH --time=00:15:00
#SBATCH --output=/data/user/caubet_m/tmp/get_cpu/logs/get_cpu-%j.out
#SBATCH --error=/data/user/caubet_m/tmp/get_cpu/logs/get_cpu-%j.err
#SBATCH --ntasks=2
#SBATCH --hint=nomultithread
#SBATCH --cpus-per-task=10

export TMPDIR=/data/user/caubet_m/tmp
export LOGSDIR=${TMPDIR}/get_cpu/logs
export OUTDIR=${TMPDIR}/get_cpu/testmpi

SRUN=yes              # Recommended Default: yes
PMODULES_UNSTABLE=no  # Recommended Default: no
DELETE_OUTDIR=yes     # Recommended Default: yes
DELETE_LOGSDIR=no     # Recommended Default: no

OPENMPI_VERSION=4.0.4_slurm

EXE="/data/user/caubet_m/c_software/get_cpu/get_cpu"

# Optional delete temporary directories
if [ "${DELETE_OUTDIR}" == "yes" ]; then mkdir -p ${OUTDIR}; fi
if [ "${DELETE_LOGSDIR}" == "yes" ]; then mkdir -p ${LOGSDIR}; fi

# Optional load unstable software repository
if [ "${PMODULES_UNSTABLE}" == "yes" ]; then module use unstable; fi

for module in $(module search openmpi 2>&1  | grep "$OPENMPI_VERSION" | awk '{print $1";"$4}')
do
  compiler_module=$(echo ${module} | awk -F';' '{print $2}')
  openmpi_module=$(echo ${module} | awk -F';' '{print $1}')

  compiler=$(echo ${compiler_module} | sed 's/\//V/g')
  openmpi=$(echo ${openmpi_module} | sed 's/\//V/g')

  touch ${OUTDIR}/${openmpi}.${compiler}.log 

  module purge
  module load ${compiler_module} ${openmpi_module}
  module list 2>&1 >> ${OUTDIR}/${openmpi}.${compiler}.log
  
  echo " " >> ${OUTDIR}/${openmpi}.${compiler}.log
  make     >> ${OUTDIR}/${openmpi}.${compiler}.log
  echo " " >> ${OUTDIR}/${openmpi}.${compiler}.log

  module list
  if [ "${SRUN}" == "yes" ]
  then
    echo "# srun"
    echo "# srun" >> ${OUTDIR}/${openmpi}.${compiler}.log
    srun --cpu-bind=threads,verbose $EXE >> ${OUTDIR}/${openmpi}.${compiler}.log
  else
    echo "# mpirun"
    echo "# mpirun" >> ${OUTDIR}/${openmpi}.${compiler}.log
    ##############################################################################################################################################
    # GOOD MPI SETUP
    # export MPIRUN_OPTIONS="--bind-to core --map-by socket:PE=$SLURM_CPUS_PER_TASK --rank-by hwthread -report-bindings -mca pml ucx --mca btl ^vader,tcp,openib,uct -x UCX_NET_DEVICES=mlx5_0:1 -x UCX_LOG_LEVEL=data -x UCX_LOG_FILE=UCX.log"
    export MPIRUN_OPTIONS="--bind-to core --map-by socket:PE=$SLURM_CPUS_PER_TASK --rank-by slot -report-bindings -mca pml ucx --mca btl ^vader,tcp,openib,uct -x UCX_NET_DEVICES=mlx5_0:1 -x UCX_LOG_LEVEL=data -x UCX_LOG_FILE=UCX.log"
    export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
    export OMP_PROC_BIND=false
    mpirun -np $SLURM_NTASKS $MPIRUN_OPTIONS $EXE >> testmpi/${openmpi}.${compiler}.log
    ##############################################################################################################################################
  fi
done
