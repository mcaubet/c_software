#!/bin/bash -l 
#SBATCH --job-name=testmpi
#SBATCH --error=testmpi.error
#SBATCH --output=testmpi.out
#SBATCH --time=00:02:00
#SBATCH --gpus=2
#SBATCH --nodes=1
#SBATCH --ntasks=2
#SBATCH --ntasks-per-node=2
#SBATCH --gpus-per-task=1
#SBATCH --account=merlin
#SBATCH --exclusive
#SBATCH --partition=gpu-short
#SBATCH --cpus-per-gpu=4

module load gcc/8.4.0 openmpi/4.0.5_slurm

export OMPI_MCA_pml="ucx"
# export UCX_TLS=
export OMPI_MCA_btl="^vader,tcp,openib,uct"
# export UCX_NET_DEVICES=mlx5_0:1
export UCX_TLS=rc,cuda,shm
export UCX_LOG_LEVEL=TRACE
# export UCX_LOG_FILE=UCX-$SLURM_JOB_ID-$SLURM_NODEID.log
# export UCX_MEMTYPE_CACHE=n

echo "# ldd $(which mpirun)"
ldd $(which mpirun)
echo

echo "# ldd ./cuda_mpi"
ldd ./cuda_mpi
echo

mpirun ./cuda_mpi
